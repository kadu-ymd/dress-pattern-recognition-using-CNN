{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lista para lembrar\n",
    "\n",
    "- Fazer um `pip freeze` para os `requirements.txt`\n",
    "\n",
    "- usar a pasta `manipulated_data` criado no nosso repositório\n",
    "\n",
    "- precisa baixar as imagens do *dataset* completo para criar a pasta com as imagens que serão usadas para teste\n",
    "\n",
    "- precisa do arquivo `used_clothes.csv` do nosso repositório"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importando bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import das bibliotecas (usando PyTorch)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets.folder import default_loader\n",
    "from torchvision.datasets import DatasetFolder\n",
    "from torchvision.datasets import ImageFolder\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função equivalente ao create_model()\n",
    "\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, num_categories):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(32, 32, kernel_size=3)\n",
    "        self.fc1 = nn.Linear(32 * 14 * 14, 128)  # ajustado para entrada 64x64\n",
    "        self.fc2 = nn.Linear(128, num_categories)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))      # Conv1 + ReLU\n",
    "        x = self.pool(x)               # MaxPool1\n",
    "        x = F.relu(self.conv2(x))      # Conv2 + ReLU\n",
    "        x = self.pool(x)               # MaxPool2\n",
    "        x = x.view(-1, 32 * 14 * 14)   # Flatten\n",
    "        x = F.relu(self.fc1(x))        # Dense1 + ReLU\n",
    "        x = self.fc2(x)                # Dense2 (Logits)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função equivalente ao fitting()\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "def get_dataloaders(train_dir, test_df, test_dir, batch_size=16):\n",
    "    train_dataset = datasets.ImageFolder(train_dir, transform=transform)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Dataset personalizado para testar\n",
    "    class TestDataset(torch.utils.data.Dataset):\n",
    "        def __init__(self, dataframe, img_dir, transform=None):\n",
    "            self.dataframe = dataframe\n",
    "            self.img_dir = img_dir\n",
    "            self.transform = transform\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.dataframe)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            img_name = self.dataframe.iloc[idx, 0]\n",
    "            img_path = os.path.join(self.img_dir, img_name)\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            return image\n",
    "\n",
    "    test_dataset = TestDataset(test_df, test_dir, transform=transform)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, test_loader, train_dataset.class_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para treinamento do modelo\n",
    "\n",
    "def train_model(model, train_loader, num_epochs=5, learning_rate=0.001, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
    "    model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    history = {\"loss\": [], \"accuracy\": []}\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        epoch_acc = correct / total\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {epoch_loss:.4f} - Accuracy: {epoch_acc:.4f}\")\n",
    "        history[\"loss\"].append(epoch_loss)\n",
    "        history[\"accuracy\"].append(epoch_acc)\n",
    "\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funções para salvar o modelo\n",
    "\n",
    "def save_model(model, model_name=\"model.pth\"):\n",
    "    torch.save(model.state_dict(), model_name)\n",
    "    print(f\"-> Model saved as {model_name}\")\n",
    "\n",
    "def save_history(history, filename=\"history.json\"):\n",
    "    with open(filename, \"w\") as f:\n",
    "        json.dump(history, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funções em comum com a versão utilizando TensorFlow\n",
    "\n",
    "def create_train_dict(csv_filename: str):\n",
    "    df_file = pd.read_csv(csv_filename)\n",
    "\n",
    "    try:\n",
    "        df_file.drop(\"Unnamed: 0\", axis=1, inplace=True)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    df_file1 = df_file.loc[:, df_file.columns.isin([\"file_name\", \"Details\"])]\n",
    "\n",
    "    rel_dict = {}\n",
    "\n",
    "    df_file_dict = df_file1.to_dict()\n",
    "\n",
    "    for i in range(len(df_file_dict[\"Details\"])):\n",
    "        rel_dict[df_file_dict[\"file_name\"][i]] = df_file_dict[\"Details\"][i]\n",
    "\n",
    "    return rel_dict, df_file1\n",
    "\n",
    "def create_folder(folder_path):\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "\n",
    "        print(f\"-> Folder {folder_path} created successfully\")\n",
    "    else:\n",
    "        print(f\"-> Folder {folder_path} already exists\")\n",
    "\n",
    "def create_folder_train_dataset(rel_dict: dict, foldername: str, src_path: str = \"images\"):\n",
    "    for img, category in rel_dict.items():\n",
    "        category1 = category.lower().replace(\"-\", \"_\")\n",
    "        \n",
    "        src = os.path.join(src_path, img)\n",
    "        dest = os.path.join(f\"{foldername}/{category1}\", img)\n",
    "\n",
    "        if os.path.exists(src):\n",
    "            shutil.copy(src, dest)\n",
    "        else:\n",
    "            print(f\"-> File not found: {img}\")\n",
    "\n",
    "def create_test_dataset(csv_filename: str, default_csv_filename: str = \"manipulated_data/initial_filtered_clothes.csv\", images_path: str = \"images\") -> list:\n",
    "\n",
    "    file_list = pd.read_csv(csv_filename)[\"file_name\"].tolist()\n",
    "    default_list = pd.read_csv(default_csv_filename)[\"file_name\"].tolist()\n",
    "\n",
    "    test_clothes_list = []\n",
    "\n",
    "    for img in os.listdir(images_path):\n",
    "        if img not in file_list and img in default_list:\n",
    "            test_clothes_list.append(img)\n",
    "\n",
    "    return test_clothes_list[:len(file_list)//2]\n",
    "\n",
    "def create_folder_dataset(test_dataset: list, dest_path: str, src_path: str = \"images\") -> pd.DataFrame:\n",
    "    if not os.path.exists(dest_path):\n",
    "        os.makedirs(dest_path)\n",
    "\n",
    "        print(f\"-> Folder {dest_path} created successfully\")\n",
    "    else:\n",
    "        print(f\"-> Folder {dest_path} already exists\")\n",
    "\n",
    "    for img in test_dataset:\n",
    "        src = os.path.join(src_path, img)\n",
    "        dest = os.path.join(dest_path, img)\n",
    "\n",
    "        if os.path.exists(src):\n",
    "            shutil.copy(src, dest)\n",
    "        else:\n",
    "            print(f\"-> File not found: {img}\")\n",
    "    \n",
    "    return pd.DataFrame({\"file_name\": test_dataset})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nome dos arquivos CSV a serem utilizados\n",
    "\n",
    "csv_list = []\n",
    "\n",
    "csv_path = \"csv_files\"\n",
    "\n",
    "if os.path.exists(\"csv_files\"):\n",
    "    for filename in os.listdir(\"csv_files\"):\n",
    "        if re.match(r\"\\d+_details_categories\\.csv\", filename):\n",
    "            csv_list.append(f\"{csv_path}/{filename}\")\n",
    "        else:\n",
    "            print(f\"Arquivo {filename} possui nome que não segue o padrão exigido (num_details_categories.csv\")\n",
    "else:\n",
    "    print(\"Caminho para arquivos não existe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop principal (single-thread)\n",
    "\n",
    "history_list = []\n",
    "\n",
    "for csv_file in csv_list:\n",
    "    train_dict, df_file1 = create_train_dict(csv_file)\n",
    "    categories_list = [cat.lower().replace(\"-\", \"_\") for cat in df_file1[\"Details\"].value_counts().keys()]\n",
    "    num_classes = len(categories_list)\n",
    "\n",
    "    dataset_train_path = f\"dataset_train_{csv_file[0]}\"\n",
    "    dataset_test_path = f\"dataset_test_{csv_file[0]}\"\n",
    "    test_dataset = create_test_dataset(csv_file)\n",
    "    test_dataframe = create_folder_dataset(test_dataset, dataset_test_path)\n",
    "\n",
    "    train_loader, test_loader, class_map = get_dataloaders(\n",
    "        dataset_train_path, test_dataframe, dataset_test_path\n",
    "    )\n",
    "\n",
    "    model = CNNModel(num_categories=num_classes)\n",
    "    model, history = train_model(model, train_loader)\n",
    "\n",
    "    history_list.append(history)\n",
    "    save_model(model, f\"model_{csv_file[0]}.pth\")\n",
    "    save_history(history, f\"model_{csv_file[0]}_summary.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Folder dataset_test_2 already exists\n",
      "Epoch 1/5 - Loss: 0.6699 - Accuracy: 0.5741\n",
      "Epoch 2/5 - Loss: 0.6200 - Accuracy: 0.6468\n",
      "Epoch 3/5 - Loss: 0.5769 - Accuracy: 0.6891\n",
      "Epoch 4/5 - Loss: 0.5259 - Accuracy: 0.7300\n",
      "Epoch 5/5 - Loss: 0.5111 - Accuracy: 0.7344\n",
      "-> Model saved as model_2.pth\n",
      "-> Folder dataset_test_3 already exists\n",
      "Epoch 1/5 - Loss: 1.0186 - Accuracy: 0.4990\n",
      "Epoch 2/5 - Loss: 0.9510 - Accuracy: 0.5470\n",
      "Epoch 3/5 - Loss: 0.9143 - Accuracy: 0.5710\n",
      "Epoch 4/5 - Loss: 0.8575 - Accuracy: 0.6168\n",
      "Epoch 5/5 - Loss: 0.7950 - Accuracy: 0.6407\n",
      "-> Model saved as model_3.pth\n",
      "-> Folder dataset_test_6 already exists\n",
      "Epoch 1/5 - Loss: 1.6940 - Accuracy: 0.2691\n",
      "Epoch 2/5 - Loss: 1.5822 - Accuracy: 0.3419\n",
      "Epoch 3/5 - Loss: 1.4569 - Accuracy: 0.4181\n",
      "Epoch 4/5 - Loss: 1.3145 - Accuracy: 0.4871\n",
      "Epoch 5/5 - Loss: 1.1961 - Accuracy: 0.5436\n",
      "-> Model saved as model_6.pth\n"
     ]
    }
   ],
   "source": [
    "# Loop principal (multi-thread)\n",
    "\n",
    "history_list = []\n",
    "\n",
    "for csv_file in csv_list:\n",
    "    train_dict, df_file1 = create_train_dict(csv_file)\n",
    "    categories_list = [cat.lower().replace(\"-\", \"_\") for cat in df_file1[\"Details\"].value_counts().keys()]\n",
    "    num_classes = len(categories_list)\n",
    "\n",
    "    dataset_train_path = f\"dataset_train_{csv_file[10]}\"\n",
    "    dataset_test_path = f\"dataset_test_{csv_file[10]}\"\n",
    "    test_dataset = create_test_dataset(csv_file)\n",
    "    test_dataframe = create_folder_dataset(test_dataset, dataset_test_path)\n",
    "\n",
    "    train_dataset = ImageFolder(root=dataset_train_path, transform=transform)\n",
    "    class_map = train_dataset.class_to_idx\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "    model = CNNModel(num_categories=num_classes)\n",
    "    model, history = train_model(model, train_loader)\n",
    "\n",
    "    history_list.append(history)\n",
    "    save_model(model, f\"model_{csv_file[10]}.pth\")\n",
    "    save_history(history, f\"model_{csv_file[10]}_summary.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Versão tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Folder dataset_train_2 already exists\n",
      "-> Folder dataset_train_2/solid already exists\n",
      "-> Folder dataset_train_2/non_solid already exists\n",
      "-> Folder dataset_test_2 already exists\n",
      "Found 5011 images belonging to 2 classes.\n",
      "Found 2505 validated image filenames.\n",
      "Epoch 1/5\n",
      "800/800 [==============================] - 181s 226ms/step - loss: 0.4627 - accuracy: 0.7737\n",
      "Epoch 2/5\n",
      "800/800 [==============================] - 146s 182ms/step - loss: 0.3498 - accuracy: 0.8450\n",
      "Epoch 3/5\n",
      "800/800 [==============================] - 157s 196ms/step - loss: 0.2210 - accuracy: 0.9097\n",
      "Epoch 4/5\n",
      "800/800 [==============================] - 156s 195ms/step - loss: 0.1237 - accuracy: 0.9523\n",
      "Epoch 5/5\n",
      "800/800 [==============================] - 147s 184ms/step - loss: 0.0679 - accuracy: 0.9751\n",
      "-> Model saved successfuly in file model_dataset_test_2.h5\n",
      "-> Folder dataset_train_3 already exists\n",
      "-> Folder dataset_train_3/pattern already exists\n",
      "-> Folder dataset_train_3/solid already exists\n",
      "-> Folder dataset_train_3/geometric already exists\n",
      "-> Folder dataset_test_3 already exists\n",
      "Found 4932 images belonging to 3 classes.\n",
      "Found 2915 validated image filenames.\n",
      "Epoch 1/5\n",
      "800/800 [==============================] - 146s 182ms/step - loss: 0.8261 - accuracy: 0.6251\n",
      "Epoch 2/5\n",
      "800/800 [==============================] - 143s 179ms/step - loss: 0.6003 - accuracy: 0.7480\n",
      "Epoch 3/5\n",
      "800/800 [==============================] - 148s 185ms/step - loss: 0.3317 - accuracy: 0.8712\n",
      "Epoch 4/5\n",
      "800/800 [==============================] - 150s 188ms/step - loss: 0.1518 - accuracy: 0.9466\n",
      "Epoch 5/5\n",
      "800/800 [==============================] - 154s 192ms/step - loss: 0.0909 - accuracy: 0.9730\n",
      "-> Model saved successfuly in file model_dataset_test_3.h5\n",
      "-> Folder dataset_train_6 already exists\n",
      "-> Folder dataset_train_6/pattern already exists\n",
      "-> Folder dataset_train_6/floral already exists\n",
      "-> Folder dataset_train_6/solid already exists\n",
      "-> Folder dataset_train_6/stripes already exists\n",
      "-> Folder dataset_train_6/checkers already exists\n",
      "-> Folder dataset_train_6/animal print already exists\n",
      "-> Folder dataset_test_6 already exists\n",
      "Found 6845 images belonging to 6 classes.\n",
      "Found 3422 validated image filenames.\n",
      "Epoch 1/5\n",
      "800/800 [==============================] - 151s 189ms/step - loss: 1.5134 - accuracy: 0.3703\n",
      "Epoch 2/5\n",
      "800/800 [==============================] - 140s 175ms/step - loss: 1.1056 - accuracy: 0.5825\n",
      "Epoch 3/5\n",
      "800/800 [==============================] - 141s 176ms/step - loss: 0.7162 - accuracy: 0.7397\n",
      "Epoch 4/5\n",
      "800/800 [==============================] - 154s 193ms/step - loss: 0.4252 - accuracy: 0.8540\n",
      "Epoch 5/5\n",
      "800/800 [==============================] - 144s 180ms/step - loss: 0.2354 - accuracy: 0.9221\n",
      "-> Model saved successfuly in file model_dataset_test_6.h5\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "write() argument must be str, not dict",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_25936\\3317687569.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[0msave_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mf\"dataset_test_{csv_file[0]}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m \u001b[0msave_history\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_25936\\4177727240.py\u001b[0m in \u001b[0;36msave_history\u001b[1;34m(history_list)\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhistory\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"model_{idx}.json\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"w\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 132\u001b[1;33m             \u001b[0mfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: write() argument must be str, not dict"
     ]
    }
   ],
   "source": [
    "history_list = []\n",
    "\n",
    "for csv_file in csv_list:\n",
    "    # Treino\n",
    "    train_dict, df_file1 = create_train_dict(csv_file)\n",
    "\n",
    "    create_folder(f\"dataset_train_{csv_file[0]}\")\n",
    "\n",
    "    categories_list = [cat.lower().replace(\"-\", \"_\") for cat in df_file1[\"Details\"].value_counts().keys()]\n",
    "\n",
    "    for category in categories_list:\n",
    "        create_folder(f\"dataset_train_{csv_file[0]}/{category}\")\n",
    "\n",
    "    create_folder_train_dataset(train_dict, f\"dataset_train_{csv_file[0]}\")\n",
    "\n",
    "    # Teste\n",
    "    test_dataset = create_test_dataset(csv_file)\n",
    "    test_dataframe = create_folder_dataset(test_dataset, f\"dataset_test_{csv_file[0]}\")\n",
    "    \n",
    "    classifier = create_model(num_categories=int(csv_file[0])) # mudar para receber o tamanho da camada Dense\n",
    "\n",
    "    training_set, test_set = fitting(test_dataframe, train_image_path=f\"dataset_train_{csv_file[0]}\", test_image_path=f\"dataset_test_{csv_file[0]}\")\n",
    "    \n",
    "    history = classifier.fit(training_set, steps_per_epoch=800, epochs=5)\n",
    "\n",
    "    history_list.append(history)\n",
    "    \n",
    "    save_model(classifier, f\"dataset_test_{csv_file[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salva o sumário do modelo em um arquivo JSON\n",
    "for idx, h in enumerate(history_list):\n",
    "    h.history[\"loss\"] = [float(item) for item in h.history[\"loss\"]]\n",
    "    h.history[\"accuracy\"] = [float(item) for item in h.history[\"accuracy\"]]\n",
    "\n",
    "    with open(f\"model_{idx}_summary.json\", \"w\") as json_file:\n",
    "        json.dump(h.history, json_file, indent=4, ensure_ascii=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5011 images belonging to 2 classes.\n",
      "Found 4932 images belonging to 3 classes.82684c2d96fc771379fb354e.jpg (2505/2505) -> 100.00%\n",
      "Found 6845 images belonging to 6 classes.26a44fa4b182af3f86968d89.jpg (2915/2915) -> 100.00%\n",
      "Dataset: dataset_test_6; Imagem: 1d21c9f90e524b2dbf310d632b8659ab.jpg (3422/3422) -> 100.00%\r"
     ]
    }
   ],
   "source": [
    "predictions_list = []\n",
    "\n",
    "for i in [2, 3, 6]:\n",
    "    aux_dict = {}\n",
    "\n",
    "    data_path = f\"dataset_test_{i}\"\n",
    "    model_path = f\"model_dataset_test_{i}.h5\"\n",
    "    train_path = f\"dataset_train_{i}\"\n",
    "\n",
    "    classifier = create_model(num_categories=i)\n",
    "    classifier.load_weights(model_path)\n",
    "\n",
    "    datagen = ImageDataGenerator(rescale=1./255)\n",
    "    generator = datagen.flow_from_directory(\n",
    "        train_path,\n",
    "        target_size=(64, 64),\n",
    "        batch_size=1,\n",
    "        class_mode='categorical'\n",
    "    )\n",
    "\n",
    "    class_indices = generator.class_indices\n",
    "    inv_class_indices = {v: k for k, v in class_indices.items()}\n",
    "\n",
    "    dset_size = len(os.listdir(data_path))\n",
    "\n",
    "    for idx, img in enumerate(os.listdir(data_path)):\n",
    "        img_path = os.path.join(data_path, img)\n",
    "\n",
    "        test_image = image.load_img(img_path, target_size = (64, 64))\n",
    "        test_image = image.img_to_array(test_image)\n",
    "        test_image = np.expand_dims(test_image, axis = 0)\n",
    "\n",
    "        result = classifier.predict(test_image)\n",
    "\n",
    "        index = np.argmax(result[0])\n",
    "        prediction = inv_class_indices[index]\n",
    "\n",
    "        if prediction in aux_dict:\n",
    "            aux_dict[prediction].append(img)\n",
    "        else:\n",
    "            aux_dict[prediction] = [img]\n",
    "\n",
    "        print(f\"Dataset: {data_path}; Imagem: {img} ({idx+1}/{dset_size}) -> {((idx+1)/dset_size) * 100:.2f}%\", end=\"\\r\")\n",
    "        print(\"\\n\" + \"-\" * 100)\n",
    "    \n",
    "    predictions_list.append(aux_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env.torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
