{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Versão PyTorch\n",
    "\n",
    "Essa versão foi feita entre os dias 21/05 e 22/05."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importando bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from torchvision.transforms import Compose\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criando pastas e definindo dataset de imagens a serem utilizadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_files_path = \"csv_files\"\n",
    "csv_filename = \"2_details_categories.csv\"\n",
    "src_image_path = \"preprocessed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = {\n",
    "    \"2_details_categories.csv\": 2,\n",
    "    \"3_details_categories.csv\": 3,\n",
    "    \"6_details_categories.csv\": 6\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treinando o modelo\n",
    "\n",
    "Instanciando o modelo utilizado (classe `CNNModel`) com a quantidade de categorias que desejamos utilizar (2, 3 ou 6)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criação do dataset (`ImageFolder`) e separação em train ($80\\%$) e test ($20\\%$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Treinamento do modelo utilizando 10 *epochs*. O modelo é salvo em um arquivo *.pth* que pode ser carregado posteriormente para ser testado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"model_2\": [CNNModel(2), \"model_weights_2.pth\"],\n",
    "    \"model_3\": [CNNModel(3), \"model_weights_3.pth\"],\n",
    "    \"model_6\": [CNNModel(6), \"model_weights_6.pth\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo para 2 categorias salvo com sucesso em 'model_weights_2.pth'!\n",
      "Modelo para 3 categorias salvo com sucesso em 'model_weights_3.pth'!\n",
      "Modelo para 6 categorias salvo com sucesso em 'model_weights_6.pth'!\n"
     ]
    }
   ],
   "source": [
    "for file, num_categories in filenames.items():\n",
    "    df = dataframe_create(csv_files_path, file, src_image_path)\n",
    "\n",
    "    dataset_categories_create(df, f\"dataset_{num_categories}\")\n",
    "\n",
    "    dataset_create(df, f\"dataset_{num_categories}\", src_image_path)\n",
    "\n",
    "    train_loader, test_loader = train_model(num_categories, transform)\n",
    "\n",
    "    models[f\"model_{num_categories}\"].append((train_loader, test_loader))\n",
    "\n",
    "    print(f\"Modelo para {num_categories} categorias salvo com sucesso em 'model_weights_{num_categories}.pth'!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testando o modelo\n",
    "\n",
    "Para cada modelo, utilizando o respectivo `test_loader`, podemos salvar os resultados da precisão em uma lista."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_precisao = []\n",
    "\n",
    "for model, info in models.items():\n",
    "    lista_precisao.append(test_model(info[0], info[1], info[2][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'model_weights_2.pth': 88.54054054054055}, {'model_weights_3.pth': 82.6086956521739}, {'model_weights_6.pth': 82.00161420500403}]\n"
     ]
    }
   ],
   "source": [
    "print(lista_precisao)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultados\n",
    "\n",
    "Precisão dos modelos:\n",
    "\n",
    "- Para 2 categorias de estampa: $88.54\\%$\n",
    "- Para 3 categorias de estampa: $82.61\\%$\n",
    "- Para 6 categorias de estampa: $82.00\\%$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funções não mais utilizadas\n",
    "\n",
    "Estão aqui apenas para registro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Função equivalente ao create_model()\n",
    "\n",
    "# class CNNModel(nn.Module):\n",
    "#     def __init__(self, num_categories):\n",
    "#         super(CNNModel, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3)\n",
    "#         self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "#         self.conv2 = nn.Conv2d(32, 32, kernel_size=3)\n",
    "#         self.fc1 = nn.Linear(32 * 14 * 14, 128)  # ajustado para entrada 64x64\n",
    "#         self.fc2 = nn.Linear(128, num_categories)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = F.relu(self.conv1(x))      # Conv1 + ReLU\n",
    "#         x = self.pool(x)               # MaxPool1\n",
    "#         x = F.relu(self.conv2(x))      # Conv2 + ReLU\n",
    "#         x = self.pool(x)               # MaxPool2\n",
    "#         x = x.view(-1, 32 * 14 * 14)   # Flatten\n",
    "#         x = F.relu(self.fc1(x))        # Dense1 + ReLU\n",
    "#         x = self.fc2(x)                # Dense2 (Logits)\n",
    "#         return x\n",
    "\n",
    "# # Função equivalente ao fitting()\n",
    "\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.Resize((64, 64)),\n",
    "#     transforms.ToTensor()\n",
    "# ])\n",
    "\n",
    "# def get_dataloaders(train_dir, test_df, test_dir, batch_size=16):\n",
    "#     train_dataset = datasets.ImageFolder(train_dir, transform=transform)\n",
    "#     train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "#     # Dataset personalizado para testar\n",
    "#     class TestDataset(torch.utils.data.Dataset):\n",
    "#         def __init__(self, dataframe, img_dir, transform=None):\n",
    "#             self.dataframe = dataframe\n",
    "#             self.img_dir = img_dir\n",
    "#             self.transform = transform\n",
    "\n",
    "#         def __len__(self):\n",
    "#             return len(self.dataframe)\n",
    "\n",
    "#         def __getitem__(self, idx):\n",
    "#             img_name = self.dataframe.iloc[idx, 0]\n",
    "#             img_path = os.path.join(self.img_dir, img_name)\n",
    "#             image = Image.open(img_path).convert(\"RGB\")\n",
    "#             if self.transform:\n",
    "#                 image = self.transform(image)\n",
    "#             return image\n",
    "\n",
    "#     test_dataset = TestDataset(test_df, test_dir, transform=transform)\n",
    "#     test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "#     return train_loader, test_loader, train_dataset.class_to_idx\n",
    "# # Função para treinamento do modelo\n",
    "\n",
    "# def train_model(model, train_loader, num_epochs=5, learning_rate=0.001, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
    "#     model.to(device)\n",
    "#     criterion = nn.CrossEntropyLoss()\n",
    "#     optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "#     history = {\"loss\": [], \"accuracy\": []}\n",
    "\n",
    "#     for epoch in range(num_epochs):\n",
    "#         model.train()\n",
    "#         running_loss = 0.0\n",
    "#         correct = 0\n",
    "#         total = 0\n",
    "\n",
    "#         for inputs, labels in train_loader:\n",
    "#             inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "#             optimizer.zero_grad()\n",
    "#             outputs = model(inputs)\n",
    "\n",
    "#             loss = criterion(outputs, labels)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "#             running_loss += loss.item()\n",
    "#             _, predicted = torch.max(outputs.data, 1)\n",
    "#             total += labels.size(0)\n",
    "#             correct += (predicted == labels).sum().item()\n",
    "\n",
    "#         epoch_loss = running_loss / len(train_loader)\n",
    "#         epoch_acc = correct / total\n",
    "\n",
    "#         print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {epoch_loss:.4f} - Accuracy: {epoch_acc:.4f}\")\n",
    "#         history[\"loss\"].append(epoch_loss)\n",
    "#         history[\"accuracy\"].append(epoch_acc)\n",
    "\n",
    "#     return model, history\n",
    "# # Funções para salvar o modelo\n",
    "\n",
    "# def save_model(model, model_name=\"model.pth\"):\n",
    "#     torch.save(model.state_dict(), model_name)\n",
    "#     print(f\"-> Model saved as {model_name}\")\n",
    "\n",
    "# def save_history(history, filename=\"history.json\"):\n",
    "#     with open(filename, \"w\") as f:\n",
    "#         json.dump(history, f, indent=4)\n",
    "# # Funções em comum com a versão utilizando TensorFlow\n",
    "\n",
    "# def create_train_dict(csv_filename: str):\n",
    "#     df_file = pd.read_csv(csv_filename)\n",
    "\n",
    "#     lista = []\n",
    "#     for i, row in df_file.iterrows():\n",
    "#         if os.path.isfile(f\"./preprocessed/{row['file_name']}\"):\n",
    "#             lista.append(row)\n",
    "\n",
    "#     df_file = pd.DataFrame(lista)\n",
    "\n",
    "\n",
    "#     try:\n",
    "#         df_file.drop(\"Unnamed: 0\", axis=1, inplace=True)\n",
    "#     except:\n",
    "#         pass\n",
    "\n",
    "#     df_file1 = df_file.loc[:, df_file.columns.isin([\"file_name\", \"Details\"])]\n",
    "\n",
    "#     rel_dict = {}\n",
    "\n",
    "#     df_file_dict = df_file1.to_dict()\n",
    "\n",
    "#     # for i in range(len(df_file_dict[\"Details\"])):\n",
    "#         # rel_dict[df_file_dict[\"file_name\"][i]] = df_file_dict[\"Details\"][i]\n",
    "\n",
    "#     return rel_dict, df_file1\n",
    "\n",
    "# def create_folder(folder_path):\n",
    "#     if not os.path.exists(folder_path):\n",
    "#         os.makedirs(folder_path)\n",
    "\n",
    "#         print(f\"-> Folder {folder_path} created successfully\")\n",
    "#     else:\n",
    "#         print(f\"-> Folder {folder_path} already exists\")\n",
    "\n",
    "# def create_test_dataset(csv_filename: str, default_csv_filename: str = \"manipulated_data/initial_filtered_clothes.csv\", images_path: str = \"images\") -> list:\n",
    "#     file_list = pd.read_csv(csv_filename)[\"file_name\"].tolist()\n",
    "#     default_list = pd.read_csv(default_csv_filename)[\"file_name\"].tolist()\n",
    "\n",
    "#     test_clothes_list = []\n",
    "\n",
    "#     for img in os.listdir(images_path):\n",
    "#         if img not in file_list and img in default_list:\n",
    "#             test_clothes_list.append(img)\n",
    "\n",
    "#     return test_clothes_list[:len(file_list)//2]\n",
    "\n",
    "# def create_folder_dataset(test_dataset: list, dest_path: str, src_path: str = \"images\") -> pd.DataFrame:\n",
    "#     if not os.path.exists(dest_path):\n",
    "#         os.makedirs(dest_path)\n",
    "\n",
    "#         print(f\"-> Folder {dest_path} created successfully\")\n",
    "#     else:\n",
    "#         print(f\"-> Folder {dest_path} already exists\")\n",
    "\n",
    "#     for img in test_dataset:\n",
    "#         src = os.path.join(src_path, img)\n",
    "#         dest = os.path.join(dest_path, img)\n",
    "\n",
    "#         if os.path.exists(src):\n",
    "#             shutil.copy(src, dest)\n",
    "#         else:\n",
    "#             print(f\"-> File not found: {img}\")\n",
    "    \n",
    "#     return pd.DataFrame({\"file_name\": test_dataset})\n",
    "# # Nome dos arquivos CSV a serem utilizados\n",
    "\n",
    "# csv_list = []\n",
    "\n",
    "# csv_path = \"csv_files\"\n",
    "\n",
    "# if os.path.exists(\"csv_files\"):\n",
    "#     for filename in os.listdir(\"csv_files\"):\n",
    "#         if re.match(r\"\\d+_details_categories\\.csv\", filename):\n",
    "#             csv_list.append(f\"{csv_path}/{filename}\")\n",
    "#         else:\n",
    "#             print(f\"Arquivo {filename} possui nome que não segue o padrão exigido (num_details_categories.csv\")\n",
    "# else:\n",
    "#     print(\"Caminho para arquivos não existe\")\n",
    "# # Loop principal (single-thread)\n",
    "\n",
    "# history_list = []\n",
    "\n",
    "# for csv_file in csv_list:\n",
    "#     train_dict, df_file1 = create_train_dict(csv_file)\n",
    "#     categories_list = [cat.lower().replace(\"-\", \"_\") for cat in df_file1[\"Details\"].value_counts().keys()]\n",
    "#     num_classes = len(categories_list)\n",
    "\n",
    "#     dataset_train_path = f\"dataset_train_{csv_file[0]}\"\n",
    "#     dataset_test_path = f\"dataset_test_{csv_file[0]}\"\n",
    "#     test_dataset = create_test_dataset(csv_file)\n",
    "#     test_dataframe = create_folder_dataset(test_dataset, dataset_test_path)\n",
    "\n",
    "#     train_loader, test_loader, class_map = get_dataloaders(\n",
    "#         dataset_train_path, test_dataframe, dataset_test_path\n",
    "#     )\n",
    "\n",
    "#     model = CNNModel(num_categories=num_classes)\n",
    "#     model, history = train_model(model, train_loader)\n",
    "\n",
    "#     history_list.append(history)\n",
    "#     save_model(model, f\"model_{csv_file[0]}.pth\")\n",
    "#     save_history(history, f\"model_{csv_file[0]}_summary.json\")\n",
    "# # Loop principal (multi-thread)\n",
    "\n",
    "# history_list = []\n",
    "\n",
    "# for csv_file in csv_list:\n",
    "#     _, df_file1 = create_train_dict(csv_file)\n",
    "#     categories_list = [cat.lower().replace(\"-\", \"_\") for cat in df_file1[\"Details\"].value_counts().keys()]\n",
    "#     num_classes = len(categories_list)\n",
    "\n",
    "#     dataset_train_path = f\"dataset_train_{csv_file[10]}\"\n",
    "#     dataset_test_path = f\"dataset_test_{csv_file[10]}\"\n",
    "#     test_dataset = create_test_dataset(csv_file)\n",
    "\n",
    "#     # train_dataset = ImageFolder(root=dataset_train_path, transform=transform)\n",
    "#     # class_map = train_dataset.class_to_idx\n",
    "\n",
    "#     # train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\n",
    "#     # test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "#     # model = CNNModel(num_categories=num_classes)\n",
    "#     # model, history = train_model(model, train_loader)\n",
    "\n",
    "#     # history_list.append(history)\n",
    "#     # save_model(model, f\"model_{csv_file[10]}.pth\")\n",
    "#     # save_history(history, f\"model_{csv_file[10]}_summary.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env.torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
