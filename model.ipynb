{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lista para lembrar\n",
    "\n",
    "- Fazer um `pip freeze` para os `requirements.txt`\n",
    "\n",
    "- usar a pasta `manipulated_data` criado no nosso repositório\n",
    "\n",
    "- precisa baixar as imagens do *dataset* completo para criar a pasta com as imagens que serão usadas para teste\n",
    "\n",
    "- precisa do arquivo `used_clothes.csv` do nosso repositório"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importando bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the Keras libraries and packages\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing import image\n",
    "\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_dict(csv_filename: str):\n",
    "    df_file = pd.read_csv(csv_filename)\n",
    "\n",
    "    try:\n",
    "        df_file.drop(\"Unnamed: 0\", axis=1, inplace=True)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    df_file1 = df_file.loc[:, df_file.columns.isin([\"file_name\", \"Details\"])]\n",
    "\n",
    "    rel_dict = {}\n",
    "\n",
    "    df_file_dict = df_file1.to_dict()\n",
    "\n",
    "    for i in range(len(df_file_dict[\"Details\"])):\n",
    "        rel_dict[df_file_dict[\"file_name\"][i]] = df_file_dict[\"Details\"][i]\n",
    "\n",
    "    return rel_dict, df_file1\n",
    "\n",
    "def create_folder(folder_path):\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "\n",
    "        print(f\"-> Folder {folder_path} created successfully\")\n",
    "    else:\n",
    "        print(f\"-> Folder {folder_path} already exists\")\n",
    "\n",
    "def create_folder_train_dataset(rel_dict: dict, foldername: str, src_path: str = \"images\"):\n",
    "    for img, category in rel_dict.items():\n",
    "        category1 = category.lower().replace(\"-\", \"_\")\n",
    "        \n",
    "        src = os.path.join(src_path, img)\n",
    "        dest = os.path.join(f\"{foldername}/{category1}\", img)\n",
    "\n",
    "        if os.path.exists(src):\n",
    "            shutil.copy(src, dest)\n",
    "        else:\n",
    "            print(f\"-> File not found: {img}\")\n",
    "\n",
    "def create_test_dataset(csv_filename: str, default_csv_filename: str = \"manipulated_data/initial_filtered_clothes.csv\", images_path: str = \"images\") -> list:\n",
    "\n",
    "    file_list = pd.read_csv(csv_filename)[\"file_name\"].tolist()\n",
    "    default_list = pd.read_csv(default_csv_filename)[\"file_name\"].tolist()\n",
    "\n",
    "    test_clothes_list = []\n",
    "\n",
    "    for img in os.listdir(images_path):\n",
    "        if img not in file_list and img in default_list:\n",
    "            test_clothes_list.append(img)\n",
    "\n",
    "    return test_clothes_list[:len(file_list)//2]\n",
    "\n",
    "def create_folder_dataset(test_dataset: list, dest_path: str, src_path: str = \"images\") -> pd.DataFrame:\n",
    "    if not os.path.exists(dest_path):\n",
    "        os.makedirs(dest_path)\n",
    "\n",
    "        print(f\"-> Folder {dest_path} created successfully\")\n",
    "    else:\n",
    "        print(f\"-> Folder {dest_path} already exists\")\n",
    "\n",
    "    for img in test_dataset:\n",
    "        src = os.path.join(src_path, img)\n",
    "        dest = os.path.join(dest_path, img)\n",
    "\n",
    "        if os.path.exists(src):\n",
    "            shutil.copy(src, dest)\n",
    "        else:\n",
    "            print(f\"-> File not found: {img}\")\n",
    "    \n",
    "    return pd.DataFrame({\"file_name\": test_dataset})\n",
    "\n",
    "def create_model(num_categories: int, summary: bool = False) -> Sequential:\n",
    "    # Criando o classificador\n",
    "    classifier = Sequential()\n",
    "\n",
    "    # Step 1 - Convolution\n",
    "    classifier.add(Conv2D(32, (3, 3), input_shape = (64, 64, 3), activation = 'relu')) # por padrão kernel_size é (3, 3)\n",
    "\n",
    "    # Step 2 - Pooling\n",
    "    classifier.add(MaxPooling2D(pool_size = (2, 2))) # por padrão pool_size é (2, 2)\n",
    "\n",
    "    # Adding a second convolutional layer\n",
    "    classifier.add(Conv2D(32, (3, 3), activation = 'relu')) # por padrão kernel_size é (3, 3)\n",
    "    classifier.add(MaxPooling2D(pool_size = (2, 2))) # por padrão pool_size é (2, 2)\n",
    "\n",
    "    # Step 3 - Flattening\n",
    "    classifier.add(Flatten())\n",
    "\n",
    "    # Step 4 - Full connection\n",
    "    classifier.add(Dense(units = 128, activation = 'relu'))\n",
    "    classifier.add(Dense(units = num_categories, activation = 'softmax'))\n",
    "\n",
    "    # Compiling the CNN\n",
    "    classifier.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "    if summary:\n",
    "        classifier.summary()\n",
    "\n",
    "    return classifier\n",
    "\n",
    "def fitting(test_dataframe: pd.DataFrame, train_image_path: str, test_image_path: str) -> tuple:\n",
    "    train_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "    test_datagen = ImageDataGenerator(rescale = 1. / 255)\n",
    "\n",
    "    training_set = train_datagen.flow_from_directory(train_image_path,\n",
    "                                                    target_size=(64, 64),\n",
    "                                                    batch_size=16,\n",
    "                                                    class_mode='categorical')\n",
    "\n",
    "    test_set = test_datagen.flow_from_dataframe(dataframe=test_dataframe,\n",
    "                                                directory=test_image_path,\n",
    "                                                x_col='file_name',\n",
    "                                                class_mode=None,\n",
    "                                                target_size=(64, 64),\n",
    "                                                batch_size=16,\n",
    "                                                shuffle=False)\n",
    "    \n",
    "    return training_set, test_set\n",
    "\n",
    "def save_model(classifier: Sequential, model_name: str):\n",
    "    model_json = classifier.to_json()\n",
    "    with open(f\"model_{model_name}.json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "\n",
    "    classifier.save_weights(f\"model_{model_name}.h5\")\n",
    "    print(f\"-> Model saved successfuly in file model_{model_name}.h5\")\n",
    "\n",
    "def save_history(history_list: list):\n",
    "    for idx, history in enumerate(history_list):\n",
    "        with open(f\"model_{idx}.json\", \"w\") as file:\n",
    "            file.write(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_list = [\"2_details_categories.csv\", \"3_details_categories.csv\", \"6_details_categories.csv\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Folder dataset_train_2 already exists\n",
      "-> Folder dataset_train_2/solid already exists\n",
      "-> Folder dataset_train_2/non_solid already exists\n",
      "-> Folder dataset_test_2 already exists\n",
      "Found 5011 images belonging to 2 classes.\n",
      "Found 2505 validated image filenames.\n",
      "Epoch 1/5\n",
      "800/800 [==============================] - 181s 226ms/step - loss: 0.4627 - accuracy: 0.7737\n",
      "Epoch 2/5\n",
      "800/800 [==============================] - 146s 182ms/step - loss: 0.3498 - accuracy: 0.8450\n",
      "Epoch 3/5\n",
      "800/800 [==============================] - 157s 196ms/step - loss: 0.2210 - accuracy: 0.9097\n",
      "Epoch 4/5\n",
      "800/800 [==============================] - 156s 195ms/step - loss: 0.1237 - accuracy: 0.9523\n",
      "Epoch 5/5\n",
      "800/800 [==============================] - 147s 184ms/step - loss: 0.0679 - accuracy: 0.9751\n",
      "-> Model saved successfuly in file model_dataset_test_2.h5\n",
      "-> Folder dataset_train_3 already exists\n",
      "-> Folder dataset_train_3/pattern already exists\n",
      "-> Folder dataset_train_3/solid already exists\n",
      "-> Folder dataset_train_3/geometric already exists\n",
      "-> Folder dataset_test_3 already exists\n",
      "Found 4932 images belonging to 3 classes.\n",
      "Found 2915 validated image filenames.\n",
      "Epoch 1/5\n",
      "800/800 [==============================] - 146s 182ms/step - loss: 0.8261 - accuracy: 0.6251\n",
      "Epoch 2/5\n",
      "800/800 [==============================] - 143s 179ms/step - loss: 0.6003 - accuracy: 0.7480\n",
      "Epoch 3/5\n",
      "800/800 [==============================] - 148s 185ms/step - loss: 0.3317 - accuracy: 0.8712\n",
      "Epoch 4/5\n",
      "800/800 [==============================] - 150s 188ms/step - loss: 0.1518 - accuracy: 0.9466\n",
      "Epoch 5/5\n",
      "800/800 [==============================] - 154s 192ms/step - loss: 0.0909 - accuracy: 0.9730\n",
      "-> Model saved successfuly in file model_dataset_test_3.h5\n",
      "-> Folder dataset_train_6 already exists\n",
      "-> Folder dataset_train_6/pattern already exists\n",
      "-> Folder dataset_train_6/floral already exists\n",
      "-> Folder dataset_train_6/solid already exists\n",
      "-> Folder dataset_train_6/stripes already exists\n",
      "-> Folder dataset_train_6/checkers already exists\n",
      "-> Folder dataset_train_6/animal print already exists\n",
      "-> Folder dataset_test_6 already exists\n",
      "Found 6845 images belonging to 6 classes.\n",
      "Found 3422 validated image filenames.\n",
      "Epoch 1/5\n",
      "800/800 [==============================] - 151s 189ms/step - loss: 1.5134 - accuracy: 0.3703\n",
      "Epoch 2/5\n",
      "800/800 [==============================] - 140s 175ms/step - loss: 1.1056 - accuracy: 0.5825\n",
      "Epoch 3/5\n",
      "800/800 [==============================] - 141s 176ms/step - loss: 0.7162 - accuracy: 0.7397\n",
      "Epoch 4/5\n",
      "800/800 [==============================] - 154s 193ms/step - loss: 0.4252 - accuracy: 0.8540\n",
      "Epoch 5/5\n",
      "800/800 [==============================] - 144s 180ms/step - loss: 0.2354 - accuracy: 0.9221\n",
      "-> Model saved successfuly in file model_dataset_test_6.h5\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "write() argument must be str, not dict",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_25936\\3317687569.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[0msave_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mf\"dataset_test_{csv_file[0]}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m \u001b[0msave_history\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_25936\\4177727240.py\u001b[0m in \u001b[0;36msave_history\u001b[1;34m(history_list)\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhistory\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"model_{idx}.json\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"w\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 132\u001b[1;33m             \u001b[0mfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: write() argument must be str, not dict"
     ]
    }
   ],
   "source": [
    "history_list = []\n",
    "\n",
    "for csv_file in csv_list:\n",
    "    # Treino\n",
    "    train_dict, df_file1 = create_train_dict(csv_file)\n",
    "\n",
    "    create_folder(f\"dataset_train_{csv_file[0]}\")\n",
    "\n",
    "    categories_list = [cat.lower().replace(\"-\", \"_\") for cat in df_file1[\"Details\"].value_counts().keys()]\n",
    "\n",
    "    for category in categories_list:\n",
    "        create_folder(f\"dataset_train_{csv_file[0]}/{category}\")\n",
    "\n",
    "    create_folder_train_dataset(train_dict, f\"dataset_train_{csv_file[0]}\")\n",
    "\n",
    "    # Teste\n",
    "    test_dataset = create_test_dataset(csv_file)\n",
    "    test_dataframe = create_folder_dataset(test_dataset, f\"dataset_test_{csv_file[0]}\")\n",
    "    \n",
    "    classifier = create_model(num_categories=int(csv_file[0])) # mudar para receber o tamanho da camada Dense\n",
    "\n",
    "    training_set, test_set = fitting(test_dataframe, train_image_path=f\"dataset_train_{csv_file[0]}\", test_image_path=f\"dataset_test_{csv_file[0]}\")\n",
    "    \n",
    "    history = classifier.fit(training_set, steps_per_epoch=800, epochs=5)\n",
    "\n",
    "    history_list.append(history)\n",
    "    \n",
    "    save_model(classifier, f\"dataset_test_{csv_file[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"a\": 0.42345\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps({\"a\": 0.42345}, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, h in enumerate(history_list):\n",
    "    h.history[\"loss\"] = [float(item) for item in h.history[\"loss\"]]\n",
    "    h.history[\"accuracy\"] = [float(item) for item in h.history[\"accuracy\"]]\n",
    "\n",
    "    with open(f\"model_{idx}_summary.json\", \"w\") as json_file:\n",
    "        json.dump(h.history, json_file, indent=4, ensure_ascii=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5011 images belonging to 2 classes.\n",
      "Found 4932 images belonging to 3 classes.82684c2d96fc771379fb354e.jpg (2505/2505) -> 100.00%\n",
      "Found 6845 images belonging to 6 classes.26a44fa4b182af3f86968d89.jpg (2915/2915) -> 100.00%\n",
      "Dataset: dataset_test_6; Imagem: 1d21c9f90e524b2dbf310d632b8659ab.jpg (3422/3422) -> 100.00%\r"
     ]
    }
   ],
   "source": [
    "predictions_list = []\n",
    "\n",
    "for i in [2, 3, 6]:\n",
    "    aux_dict = {}\n",
    "\n",
    "    data_path = f\"dataset_test_{i}\"\n",
    "    model_path = f\"model_dataset_test_{i}.h5\"\n",
    "    train_path = f\"dataset_train_{i}\"\n",
    "\n",
    "    classifier = create_model(num_categories=i)\n",
    "    classifier.load_weights(model_path)\n",
    "\n",
    "    datagen = ImageDataGenerator(rescale=1./255)\n",
    "    generator = datagen.flow_from_directory(\n",
    "        train_path,\n",
    "        target_size=(64, 64),\n",
    "        batch_size=1,\n",
    "        class_mode='categorical'\n",
    "    )\n",
    "\n",
    "    class_indices = generator.class_indices\n",
    "    inv_class_indices = {v: k for k, v in class_indices.items()}\n",
    "\n",
    "    dset_size = len(os.listdir(data_path))\n",
    "\n",
    "    for idx, img in enumerate(os.listdir(data_path)):\n",
    "        img_path = os.path.join(data_path, img)\n",
    "\n",
    "        test_image = image.load_img(img_path, target_size = (64, 64))\n",
    "        test_image = image.img_to_array(test_image)\n",
    "        test_image = np.expand_dims(test_image, axis = 0)\n",
    "\n",
    "        result = classifier.predict(test_image)\n",
    "\n",
    "        index = np.argmax(result[0])\n",
    "        prediction = inv_class_indices[index]\n",
    "\n",
    "        if prediction in aux_dict:\n",
    "            aux_dict[prediction].append(img)\n",
    "        else:\n",
    "            aux_dict[prediction] = [img]\n",
    "\n",
    "        print(f\"Dataset: {data_path}; Imagem: {img} ({idx+1}/{dset_size}) -> {((idx+1)/dset_size) * 100:.2f}%\", end=\"\\r\")\n",
    "        print(\"\\n\" + \"-\" * 100)\n",
    "    \n",
    "    predictions_list.append(aux_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
