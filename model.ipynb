{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lista para lembrar\n",
    "\n",
    "- Fazer um `pip freeze` para os `requirements.txt`\n",
    "\n",
    "- usar a pasta `manipulated_data` criado no nosso repositório\n",
    "\n",
    "- precisa baixar as imagens do *dataset* completo para criar a pasta com as imagens que serão usadas para teste\n",
    "\n",
    "- precisa do arquivo `used_clothes.csv` do nosso repositório"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importando bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the Keras libraries and packages\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing import image\n",
    "from keras.models import load_model\n",
    "\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_dict(csv_filename: str):\n",
    "    df_file = pd.read_csv(csv_filename)\n",
    "\n",
    "    try:\n",
    "        df_file.drop(\"Unnamed: 0\", axis=1, inplace=True)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    df_file1 = df_file.loc[:, df_file.columns.isin([\"file_name\", \"Details\"])]\n",
    "\n",
    "    rel_dict = {}\n",
    "\n",
    "    df_file_dict = df_file1.to_dict()\n",
    "\n",
    "    for i in range(len(df_file_dict[\"Details\"])):\n",
    "        rel_dict[df_file_dict[\"file_name\"][i]] = df_file_dict[\"Details\"][i]\n",
    "\n",
    "    return rel_dict, df_file1\n",
    "\n",
    "def create_folder(folder_path):\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "\n",
    "        print(f\"-> Folder {folder_path} created successfully\")\n",
    "    else:\n",
    "        print(f\"-> Folder {folder_path} already exists\")\n",
    "\n",
    "def create_folder_train_dataset(rel_dict: dict, foldername: str, src_path: str = \"images\"):\n",
    "    for img, category in rel_dict.items():\n",
    "        category1 = category.lower().replace(\"-\", \"_\")\n",
    "        \n",
    "        src = os.path.join(src_path, img)\n",
    "        dest = os.path.join(f\"{foldername}/{category1}\", img)\n",
    "\n",
    "        if os.path.exists(src):\n",
    "            shutil.copy(src, dest)\n",
    "        else:\n",
    "            print(f\"-> File not found: {img}\")\n",
    "\n",
    "def create_test_dataset(csv_filename: str, default_csv_filename: str = \"manipulated_data/initial_filtered_clothes.csv\", images_path: str = \"images\") -> list:\n",
    "\n",
    "    file_list = pd.read_csv(csv_filename)[\"file_name\"].tolist()\n",
    "    default_list = pd.read_csv(default_csv_filename)[\"file_name\"].tolist()\n",
    "\n",
    "    test_clothes_list = []\n",
    "\n",
    "    for img in os.listdir(images_path):\n",
    "        if img not in file_list and img in default_list:\n",
    "            test_clothes_list.append(img)\n",
    "\n",
    "    return test_clothes_list[:len(file_list)//2]\n",
    "\n",
    "def create_folder_dataset(test_dataset: list, dest_path: str, src_path: str = \"images\") -> pd.DataFrame:\n",
    "    if not os.path.exists(dest_path):\n",
    "        os.makedirs(dest_path)\n",
    "\n",
    "        print(f\"-> Folder {dest_path} created successfully\")\n",
    "    else:\n",
    "        print(f\"-> Folder {dest_path} already exists\")\n",
    "\n",
    "    for img in test_dataset:\n",
    "        src = os.path.join(src_path, img)\n",
    "        dest = os.path.join(dest_path, img)\n",
    "\n",
    "        if os.path.exists(src):\n",
    "            shutil.copy(src, dest)\n",
    "        else:\n",
    "            print(f\"-> File not found: {img}\")\n",
    "    \n",
    "    return pd.DataFrame({\"file_name\": test_dataset})\n",
    "\n",
    "def create_model(num_categories: int, summary: bool = False) -> Sequential:\n",
    "    # Criando o classificador\n",
    "    classifier = Sequential()\n",
    "\n",
    "    # Step 1 - Convolution\n",
    "    classifier.add(Conv2D(32, (3, 3), input_shape = (64, 64, 3), activation = 'relu')) # por padrão kernel_size é (3, 3)\n",
    "\n",
    "    # Step 2 - Pooling\n",
    "    classifier.add(MaxPooling2D(pool_size = (2, 2))) # por padrão pool_size é (2, 2)\n",
    "\n",
    "    # Adding a second convolutional layer\n",
    "    classifier.add(Conv2D(32, (3, 3), activation = 'relu')) # por padrão kernel_size é (3, 3)\n",
    "    classifier.add(MaxPooling2D(pool_size = (2, 2))) # por padrão pool_size é (2, 2)\n",
    "\n",
    "    # Step 3 - Flattening\n",
    "    classifier.add(Flatten())\n",
    "\n",
    "    # Step 4 - Full connection\n",
    "    classifier.add(Dense(units = 128, activation = 'relu'))\n",
    "    classifier.add(Dense(units = num_categories, activation = 'softmax'))\n",
    "\n",
    "    # Compiling the CNN\n",
    "    classifier.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "    if summary:\n",
    "        classifier.summary()\n",
    "\n",
    "    return classifier\n",
    "\n",
    "def fitting(test_dataframe: pd.DataFrame, train_image_path: str, test_image_path: str) -> tuple:\n",
    "    train_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "    test_datagen = ImageDataGenerator(rescale = 1. / 255)\n",
    "\n",
    "    training_set = train_datagen.flow_from_directory(train_image_path,\n",
    "                                                    target_size=(64, 64),\n",
    "                                                    batch_size=16,\n",
    "                                                    class_mode='categorical')\n",
    "\n",
    "    test_set = test_datagen.flow_from_dataframe(dataframe=test_dataframe,\n",
    "                                                directory=test_image_path,\n",
    "                                                x_col='file_name',\n",
    "                                                class_mode=None,\n",
    "                                                target_size=(64, 64),\n",
    "                                                batch_size=16,\n",
    "                                                shuffle=False)\n",
    "    \n",
    "    return training_set, test_set\n",
    "\n",
    "def save_model(classifier: Sequential, model_name: str):\n",
    "    model_json = classifier.to_json()\n",
    "    with open(f\"model_{model_name}.json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "\n",
    "    classifier.save_weights(f\"model_{model_name}.h5\")\n",
    "    print(f\"-> Model saved successfuly in file model_{model_name}.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_list = [\"2_details_categories.csv\", \"3_details_categories.csv\", \"6_details_categories.csv\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Folder dataset_train_2 created successfully\n",
      "-> Folder dataset_train_2/solid created successfully\n",
      "-> Folder dataset_train_2/non_solid created successfully\n",
      "-> Folder dataset_test_2 created successfully\n",
      "Found 5011 images belonging to 2 classes.\n",
      "Found 2505 validated image filenames.\n",
      "Epoch 1/5\n",
      "800/800 [==============================] - 180s 225ms/step - loss: 0.4522 - accuracy: 0.7857\n",
      "Epoch 2/5\n",
      "800/800 [==============================] - 178s 223ms/step - loss: 0.3460 - accuracy: 0.8497\n",
      "Epoch 3/5\n",
      "800/800 [==============================] - 204s 255ms/step - loss: 0.2322 - accuracy: 0.9030\n",
      "Epoch 4/5\n",
      "800/800 [==============================] - 181s 226ms/step - loss: 0.1309 - accuracy: 0.9488\n",
      "Epoch 5/5\n",
      "800/800 [==============================] - 183s 228ms/step - loss: 0.0728 - accuracy: 0.9759\n",
      "-> Model saved successfuly in file model_dataset_test_2.h5\n",
      "-> Folder dataset_train_3 created successfully\n",
      "-> Folder dataset_train_3/pattern created successfully\n",
      "-> Folder dataset_train_3/solid created successfully\n",
      "-> Folder dataset_train_3/geometric created successfully\n",
      "-> Folder dataset_test_3 created successfully\n",
      "Found 4932 images belonging to 3 classes.\n",
      "Found 2915 validated image filenames.\n",
      "Epoch 1/5\n",
      "800/800 [==============================] - 191s 239ms/step - loss: 0.8489 - accuracy: 0.6074\n",
      "Epoch 2/5\n",
      "800/800 [==============================] - 175s 218ms/step - loss: 0.6313 - accuracy: 0.7357\n",
      "Epoch 3/5\n",
      "800/800 [==============================] - 170s 212ms/step - loss: 0.3786 - accuracy: 0.8493\n",
      "Epoch 4/5\n",
      "800/800 [==============================] - 175s 218ms/step - loss: 0.1868 - accuracy: 0.9285\n",
      "Epoch 5/5\n",
      "800/800 [==============================] - 175s 219ms/step - loss: 0.1073 - accuracy: 0.9650\n",
      "-> Model saved successfuly in file model_dataset_test_3.h5\n",
      "-> Folder dataset_train_6 created successfully\n",
      "-> Folder dataset_train_6/pattern created successfully\n",
      "-> Folder dataset_train_6/floral created successfully\n",
      "-> Folder dataset_train_6/solid created successfully\n",
      "-> Folder dataset_train_6/stripes created successfully\n",
      "-> Folder dataset_train_6/checkers created successfully\n",
      "-> Folder dataset_train_6/animal print created successfully\n",
      "-> Folder dataset_test_6 created successfully\n",
      "Found 6845 images belonging to 6 classes.\n",
      "Found 3422 validated image filenames.\n",
      "Epoch 1/5\n",
      "800/800 [==============================] - 189s 236ms/step - loss: 1.5027 - accuracy: 0.3852\n",
      "Epoch 2/5\n",
      "800/800 [==============================] - 174s 218ms/step - loss: 1.0588 - accuracy: 0.5999\n",
      "Epoch 3/5\n",
      "800/800 [==============================] - 181s 226ms/step - loss: 0.6243 - accuracy: 0.7791\n",
      "Epoch 4/5\n",
      "800/800 [==============================] - 161s 201ms/step - loss: 0.3330 - accuracy: 0.8842\n",
      "Epoch 5/5\n",
      "800/800 [==============================] - 166s 208ms/step - loss: 0.1576 - accuracy: 0.9524\n",
      "-> Model saved successfuly in file model_dataset_test_6.h5\n"
     ]
    }
   ],
   "source": [
    "history_list = []\n",
    "\n",
    "for csv_file in csv_list:\n",
    "    # Treino\n",
    "    train_dict, df_file1 = create_train_dict(csv_file)\n",
    "\n",
    "    create_folder(f\"dataset_train_{csv_file[0]}\")\n",
    "\n",
    "    categories_list = [cat.lower().replace(\"-\", \"_\") for cat in df_file1[\"Details\"].value_counts().keys()]\n",
    "\n",
    "    for category in categories_list:\n",
    "        create_folder(f\"dataset_train_{csv_file[0]}/{category}\")\n",
    "\n",
    "    create_folder_train_dataset(train_dict, f\"dataset_train_{csv_file[0]}\")\n",
    "\n",
    "    # Teste\n",
    "    test_dataset = create_test_dataset(csv_file)\n",
    "    test_dataframe = create_folder_dataset(test_dataset, f\"dataset_test_{csv_file[0]}\")\n",
    "    \n",
    "    classifier = create_model(num_categories=int(csv_file[0])) # mudar para receber o tamanho da camada Dense\n",
    "\n",
    "    training_set, test_set = fitting(test_dataframe, train_image_path=f\"dataset_train_{csv_file[0]}\", test_image_path=f\"dataset_test_{csv_file[0]}\")\n",
    "    \n",
    "    history = classifier.fit(training_set, steps_per_epoch=800, epochs=5)\n",
    "\n",
    "    history_list.append(history)\n",
    "    \n",
    "    save_model(classifier, f\"dataset_test_{csv_file[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': [0.45236373018734105, 0.3455585864770376, 0.23047433626445313, 0.13111484219802455, 0.07293099807057656], 'accuracy': [0.7857367, 0.8496983, 0.90298563, 0.94882846, 0.97588855]}\n"
     ]
    }
   ],
   "source": [
    "print(history_list[0].history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5011 images belonging to 2 classes.\n",
      "Found 4932 images belonging to 3 classes.82684c2d96fc771379fb354e.jpg (2505/2505) -> 100.00%\n",
      "Found 6845 images belonging to 6 classes.26a44fa4b182af3f86968d89.jpg (2915/2915) -> 100.00%\n",
      "Dataset: dataset_test_6; Imagem: 1d21c9f90e524b2dbf310d632b8659ab.jpg (3422/3422) -> 100.00%\r"
     ]
    }
   ],
   "source": [
    "predictions_list = []\n",
    "\n",
    "for i in [2, 3, 6]:\n",
    "    aux_dict = {}\n",
    "\n",
    "    data_path = f\"dataset_test_{i}\"\n",
    "    model_path = f\"model_dataset_test_{i}.h5\"\n",
    "    train_path = f\"dataset_train_{i}\"\n",
    "\n",
    "    classifier = create_model(num_categories=i)\n",
    "    classifier.load_weights(model_path)\n",
    "\n",
    "    datagen = ImageDataGenerator(rescale=1./255)\n",
    "    generator = datagen.flow_from_directory(\n",
    "        train_path,\n",
    "        target_size=(64, 64),\n",
    "        batch_size=1,\n",
    "        class_mode='categorical'\n",
    "    )\n",
    "\n",
    "    class_indices = generator.class_indices\n",
    "    inv_class_indices = {v: k for k, v in class_indices.items()}\n",
    "\n",
    "    dset_size = len(os.listdir(data_path))\n",
    "\n",
    "    for idx, img in enumerate(os.listdir(data_path)):\n",
    "        img_path = os.path.join(data_path, img)\n",
    "\n",
    "        test_image = image.load_img(img_path, target_size = (64, 64))\n",
    "        test_image = image.img_to_array(test_image)\n",
    "        test_image = np.expand_dims(test_image, axis = 0)\n",
    "\n",
    "        result = classifier.predict(test_image)\n",
    "\n",
    "        index = np.argmax(result[0])\n",
    "        prediction = inv_class_indices[index]\n",
    "\n",
    "        if prediction in aux_dict:\n",
    "            aux_dict[prediction].append(img)\n",
    "        else:\n",
    "            aux_dict[prediction] = [img]\n",
    "\n",
    "        print(f\"Dataset: {data_path}; Imagem: {img} ({idx+1}/{dset_size}) -> {((idx+1)/dset_size) * 100:.2f}%\", end=\"\\r\")\n",
    "        print(\"\\n\" + \"-\" * 100)\n",
    "    \n",
    "    predictions_list.append(aux_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
